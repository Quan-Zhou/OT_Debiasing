{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
    "from aif360.datasets import BinaryLabelDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from functions import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=200\n",
    "e=0.01\n",
    "\n",
    "var_list=['hoursperweek','age','capitalgain','capitalloss' ,'education-num'] #\n",
    "var_dim=len(var_list)\n",
    "pa='race' #'sex'\n",
    "pa_dict={'Male':1,'Female':0,'White':1,'Black':0}\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "messydata=pd.read_csv(path+'/data/adult_csv.csv',usecols=var_list+[pa,'class'])\n",
    "messydata=messydata.rename(columns={pa:'S','class':'Y'})\n",
    "messydata['S']=messydata['S'].replace(pa_dict)\n",
    "messydata['Y']=messydata['Y'].replace({'>50K':1,'<=50K':0})\n",
    "messydata=messydata[(messydata['S']==1)|(messydata['S']==0)]\n",
    "for col in var_list+['S','Y']:\n",
    "    messydata[col]=messydata[col].astype('category')\n",
    "messydata['W']=1\n",
    "X=messydata[var_list+['S','W']].to_numpy() # [X,S,W]\n",
    "y=messydata['Y'].to_numpy() #[Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROCpostprocess:\n",
    "    def __init__(self,X_val,y_val,clf):\n",
    "        self.X_val =X_val\n",
    "        self.y_val =y_val\n",
    "        self.model = clf\n",
    "        self.ROC = self.buildROCusingval()\n",
    "\n",
    "    def buildbinarydata(self,X,y):\n",
    "        df=pd.DataFrame(np.concatenate((X,y.reshape(-1,1)), axis=1),columns=var_list+['S','W','Y'])\n",
    "        binaryLabelDataset = BinaryLabelDataset(\n",
    "                            favorable_label=1,\n",
    "                            unfavorable_label=0,\n",
    "                            df=df[var_list+['S','W','Y']], #df_test.drop('X',axis=1), #[x_list+['S','W','Y']],\n",
    "                            label_names=['Y'],\n",
    "                            instance_weights_name=['W'],\n",
    "                            protected_attribute_names=['S'],\n",
    "                            privileged_protected_attributes=[np.array([1.0])],\n",
    "                            unprivileged_protected_attributes=[np.array([0.])])\n",
    "        return binaryLabelDataset\n",
    "\n",
    "    def buildROCusingval(self):\n",
    "        dataset_val = self.buildbinarydata(self.X_val,self.y_val)\n",
    "        dataset_val_pred = dataset_val.copy(deepcopy=True)\n",
    "        dataset_val_pred.scores = self.model.predict_proba(dataset_val.features[:,0:var_dim])[:,positive_index].reshape(-1,1)\n",
    "        positive_index = 1 # positive label\n",
    "        privileged_groups = [{'S': 1}]\n",
    "        unprivileged_groups = [{'S': 0}]\n",
    "        # Metric used (should be one of allowed_metrics)\n",
    "        metric_name = \"Statistical parity difference\"\n",
    "        # Upper and lower bound on the fairness metric used\n",
    "        metric_ub = 0.05\n",
    "        metric_lb = -0.05\n",
    "        ROC = RejectOptionClassification(unprivileged_groups=unprivileged_groups, \n",
    "                                        privileged_groups=privileged_groups, \n",
    "                                        low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                                        num_class_thresh=50, num_ROC_margin=10,\n",
    "                                        metric_name=metric_name,\n",
    "                                        metric_ub=metric_ub, metric_lb=metric_lb)\n",
    "        ROC = ROC.fit(dataset_val, dataset_val_pred)\n",
    "        print(\"Optimal classification threshold (with fairness constraints) = %.4f\" % ROC.classification_threshold)\n",
    "        print(\"Optimal ROC margin = %.4f\" % ROC.ROC_margin)\n",
    "        return ROC\n",
    "\n",
    "    def postprocess(self,X_test,y_test):\n",
    "        dataset_test_pred = self.buildbinarydata(X_test,y_test).copy(deepcopy=True)\n",
    "        dataset_test_pred.scores = self.model.predict_proba(X_test[:,0:var_dim])[:,positive_index].reshape(-1,1)\n",
    "        dataset_test_pred_transf = self.ROC.predict(dataset_test_pred)\n",
    "        return dataset_test_pred_transf.convert_to_dataframe()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projpostprocess:\n",
    "    \n",
    "    def __init__(self,X_test,y_test,x_list,var_list,prediction_model):\n",
    "        self.model = prediction_model\n",
    "        self.thresh=0.2\n",
    "        self.K=200\n",
    "        self.e=0.01\n",
    "        self.x_list=x_list\n",
    "        self.var_list=var_list\n",
    "        self.var_dim=len(var_list)\n",
    "\n",
    "        df_test=pd.DataFrame(np.concatenate((X_test,y_test.reshape(-1,1)), axis=1),columns=var_list+['S','W','Y'])\n",
    "        df_test=df_test.groupby(by=var_list+['S','Y'],as_index=False).sum()\n",
    "        if len(x_list)>1:\n",
    "            df_test['X']=[tuple(df_test[x_list].values[r]) for r in range(df_test.shape[0])]\n",
    "            self.x_range=sorted(set(df_test['X']))\n",
    "            weight=list(1/(df_test[x_list].max()-df_test[x_list].min())) # because 'education-num' range from 1 to 16 while others 1 to 4\n",
    "            self.C=c_generate_higher(self.x_range,weight)\n",
    "        else:\n",
    "            df_test['X']=df_test[x_list]\n",
    "            self.x_range=sorted(set(df_test['X']))\n",
    "            self.C=c_generate(self.x_range)\n",
    "        self.df_test = df_test\n",
    "        self.var_range=list(pd.pivot_table(df_test,index=var_list,values=['S','W','Y']).index)\n",
    "        self.distribution_generator()\n",
    "# self.px,self.ptx,self.V,self.p0,self.p1 = \n",
    "\n",
    "    def distribution_generator(self):\n",
    "        bin=len(self.x_range)\n",
    "        dist=rdata_analysis(self.df_test,self.x_range,'X')\n",
    "        \n",
    "        dist['v']=[(dist['x_0'][i]-dist['x_1'][i])/dist['x'][i] for i in range(bin)]\n",
    "        \n",
    "        dist['t_x']=dist['x'] # #dist['x'] #dist['x_0']*0.5+dist['x_1']*0.5 \n",
    "        self.px=np.matrix(dist['x']).T\n",
    "        self.ptx=np.matrix(dist['t_x']).T\n",
    "        if np.any(dist['x_0']==0): \n",
    "            self.p0=np.matrix((dist['x_0']+1.0e-9)/sum(dist['x_0']+1.0e-9)).T\n",
    "        else:\n",
    "            self.p0=np.matrix(dist['x_0']).T \n",
    "        if np.any(dist['x_1']==0):\n",
    "            self.p1=np.matrix((dist['x_1']+1.0e-9)/sum(dist['x_1']+1.0e-9)).T\n",
    "        else:\n",
    "            self.p1=np.matrix(dist['x_1']).T \n",
    "        self.V=np.matrix(dist['v']).T\n",
    "        self.tv_origin=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "        # return px,ptx,V,p0,p1\n",
    "    \n",
    "    def coupling_generator(self,method,para=None):\n",
    "        if method == 'unconstrained':\n",
    "            coupling=baseline(self.C,self.e,self.px,self.ptx,self.K)\n",
    "        elif method == 'barycentre':\n",
    "            coupling=baseline(self.C,self.e,self.p0,self.p1,self.K)\n",
    "        elif method == 'partial':\n",
    "            coupling=partial_repair(self.C,self.e,self.px,self.ptx,self.V,para,self.K)\n",
    "        return coupling\n",
    "    \n",
    "    def postprocess(self,method,para=None):\n",
    "        if method == 'origin':\n",
    "            y_pred=self.model.predict(np.array(self.df_test[self.var_list]))\n",
    "            tv = self.tv_origin\n",
    "        else:\n",
    "            coupling = self.coupling_generator(method,para)\n",
    "            if (method == 'unconstrained') or (method == 'partial'):\n",
    "                y_pred=postprocess(self.df_test,coupling,self.x_list,self.x_range,self.var_list,self.var_range,self.model,self.thresh)\n",
    "                tv=assess_tv(self.df_test,coupling,self.x_range,self.x_list,self.var_list)\n",
    "            elif method == 'barycentre':\n",
    "                y_pred,tv=postprocess_bary(self.df_test,coupling,self.x_list,self.x_range,self.var_list,self.var_range,self.model,self.thresh)\n",
    "            else:\n",
    "                print('Unknown method')\n",
    "\n",
    "        di = DisparateImpact_postprocess(self.df_test,y_pred)\n",
    "        f1_macro = f1_score(self.df_test['Y'], y_pred, average='macro',sample_weight=self.df_test['W'])\n",
    "        f1_micro = f1_score(self.df_test['Y'], y_pred, average='micro',sample_weight=self.df_test['W'])\n",
    "        f1_weighted = f1_score(self.df_test['Y'], y_pred, average='weighted',sample_weight=self.df_test['W'])\n",
    "\n",
    "        new_row=pd.Series({'DI':di,'f1 macro':f1_macro,'f1 micro':f1_micro,'f1 weighted':f1_weighted,\n",
    "                           'TV distance':tv,'method':method})\n",
    "        return new_row.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "    tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "x_list=[]\n",
    "for key,val in tv_dist.items():\n",
    "    if val>0.08:\n",
    "        x_list+=[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal classification threshold (with fairness constraints) = 0.2100\n",
      "Optimal ROC margin = 0.0233\n"
     ]
    }
   ],
   "source": [
    "# train val test 4:2:4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "clf=RandomForestClassifier(max_depth=5, random_state=0).fit(X_train[:,0:var_dim],y_train)\n",
    "ROCpost = ROCpostprocess(X_val,y_val,clf)\n",
    "df_test_ROC = ROCpost.postprocess(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C,px,ptx,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Projpostprocess at 0x295c9c43760>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Projpostprocess(X_test,y_test,x_list,var_list,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame(np.concatenate((X_test,y_test.reshape(-1,1)), axis=1),columns=var_list+['S','W','Y'])\n",
    "df_test=df_test.groupby(by=var_list+['S','Y'],as_index=False).sum()\n",
    "\n",
    "if len(x_list)>1:\n",
    "    df_test['X']=[tuple(df_test[x_list].values[r]) for r in range(df_test.shape[0])]\n",
    "    x_range=sorted(set(df_test['X']))\n",
    "    weight=list(1/(df_test[x_list].max()-df_test[x_list].min())) # because 'education-num' range from 1 to 16 while others 1 to 4\n",
    "    C=c_generate_higher(x_range,weight)\n",
    "else:\n",
    "    df_test['X']=df_test[x_list]\n",
    "    x_range=sorted(set(df_test['X']))\n",
    "    C=c_generate(x_range)\n",
    "\n",
    "bin=len(x_range)\n",
    "var_range=list(pd.pivot_table(df_test,index=var_list,values=['S','W','Y']).index)\n",
    "\n",
    "\n",
    "dist=rdata_analysis(df_test,x_range,'X')\n",
    "dist['t_x']=dist['x'] # #dist['x'] #dist['x_0']*0.5+dist['x_1']*0.5 \n",
    "dist['v']=[(dist['x_0'][i]-dist['x_1'][i])/dist['x'][i] for i in range(bin)]\n",
    "px=np.matrix(dist['x']).T\n",
    "ptx=np.matrix(dist['t_x']).T\n",
    "if np.any(dist['x_0']==0): \n",
    "    p0=np.matrix((dist['x_0']+1.0e-9)/sum(dist['x_0']+1.0e-9)).T\n",
    "else:\n",
    "    p0=np.matrix(dist['x_0']).T \n",
    "if np.any(dist['x_1']==0):\n",
    "    p1=np.matrix((dist['x_1']+1.0e-9)/sum(dist['x_1']+1.0e-9)).T\n",
    "else:\n",
    "    p1=np.matrix(dist['x_1']).T \n",
    "V=np.matrix(dist['v']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=pd.DataFrame(columns=['DI','f1 macro','f1 micro','f1 weighted','TV distance','method'])\n",
    "for ignore in range(2):\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    # train val test 4:2:2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    clf=RandomForestClassifier(max_depth=5, random_state=0).fit(X_train[:,0:var_dim],y_train)\n",
    "\n",
    "    df_test=pd.DataFrame(np.concatenate((X_test,y_test.reshape(-1,1)), axis=1),columns=var_list+['S','W','Y'])\n",
    "    df_test=df_test.groupby(by=var_list+['S','Y'],as_index=False).sum()\n",
    "\n",
    "    if len(x_list)>1:\n",
    "        df_test['X']=[tuple(df_test[x_list].values[r]) for r in range(df_test.shape[0])]\n",
    "        x_range=sorted(set(df_test['X']))\n",
    "        weight=list(1/(df_test[x_list].max()-df_test[x_list].min())) # because 'education-num' range from 1 to 16 while others 1 to 4\n",
    "        C=c_generate_higher(x_range,weight)\n",
    "    else:\n",
    "        df_test['X']=df_test[x_list]\n",
    "        x_range=sorted(set(df_test['X']))\n",
    "        C=c_generate(x_range)\n",
    "\n",
    "    bin=len(x_range)\n",
    "    var_range=list(pd.pivot_table(df_test,index=var_list,values=['S','W','Y']).index)\n",
    "    dist=rdata_analysis(df_test,x_range,'X')\n",
    "    dist['t_x']=dist['x'] # #dist['x'] #dist['x_0']*0.5+dist['x_1']*0.5 \n",
    "    dist['v']=[(dist['x_0'][i]-dist['x_1'][i])/dist['x'][i] for i in range(bin)]\n",
    "    px=np.matrix(dist['x']).T\n",
    "    ptx=np.matrix(dist['t_x']).T\n",
    "    if np.any(dist['x_0']==0): \n",
    "        p0=np.matrix((dist['x_0']+1.0e-9)/sum(dist['x_0']+1.0e-9)).T\n",
    "    else:\n",
    "        p0=np.matrix(dist['x_0']).T \n",
    "    if np.any(dist['x_1']==0):\n",
    "        p1=np.matrix((dist['x_1']+1.0e-9)/sum(dist['x_1']+1.0e-9)).T\n",
    "    else:\n",
    "        p1=np.matrix(dist['x_1']).T \n",
    "    V=np.matrix(dist['v']).T\n",
    "\n",
    "    coupling_base=baseline(C,e,px,ptx,K)\n",
    "    coupling_bary=baseline(C,e,p0,p1,K)\n",
    "    coupling_part2=partial_repair(C,e,px,ptx,V,1.0e-3,K)\n",
    "    # coupling_part3=partial_repair(C,e,px,ptx,V,1.0e-3,K)\n",
    "    # coupling_total=partial_repair(C,e,px,ptx,V,1.0e-5,K)\n",
    "    # test_RW=reweighting(df_test)\n",
    "    \n",
    "    tv_base=assess_tv(df_test,coupling_base,x_range,x_list,var_list)\n",
    "    tv_part2=assess_tv(df_test,coupling_part2,x_range,x_list,var_list)\n",
    "    # tv_part3=assess_tv(df_test,coupling_part3,x_range,x_list,var_list)\n",
    "    # tv_total=assess_tv(df_test,coupling_total,x_range,x_list,var_list)\n",
    "    # tv_RW=assess_tv(test_RW,[],x_range,x_list,var_list)\n",
    "\n",
    "    y_pred=clf.predict(np.array(df_test[var_list]))\n",
    "    y_pred_base=postprocess(df_test,coupling_base,x_list,x_range,var_list,var_range,clf,thresh)\n",
    "    y_pred_bary,tv_bary=postprocess_bary(df_test,coupling_bary,x_list,x_range,var_list,var_range,clf,thresh)\n",
    "    y_pred_part2=postprocess(df_test,coupling_part2,x_list,x_range,var_list,var_range,clf,thresh)\n",
    "    # y_pred_part3=postprocess(df_test,coupling_part3,x_list,x_range,var_list,var_range,clf)\n",
    "    # y_pred_total=postprocess(df_test,coupling_total,x_list,x_range,var_list,var_range,clf)\n",
    "    # y_pred_RW=clf.predict(np.array(test_RW[var_list]))\n",
    "\n",
    "    new_row=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred),\n",
    "                        'f1 macro':f1_score(df_test['Y'], y_pred, average='macro',sample_weight=df_test['W']),\n",
    "                        'f1 micro':f1_score(df_test['Y'], y_pred, average='micro',sample_weight=df_test['W']),\n",
    "                        'f1 weighted':f1_score(df_test['Y'], y_pred, average='weighted',sample_weight=df_test['W']),\n",
    "                        'TV distance':sum(abs(dist['x_0']-dist['x_1']))/2,'method':'origin'})\n",
    "    new_row_base=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_base),\n",
    "                        'f1 macro':f1_score(df_test['Y'], y_pred_base, average='macro',sample_weight=df_test['W']),\n",
    "                        'f1 micro':f1_score(df_test['Y'], y_pred_base, average='micro',sample_weight=df_test['W']),\n",
    "                        'f1 weighted':f1_score(df_test['Y'], y_pred_base, average='weighted',sample_weight=df_test['W']),\n",
    "                        'TV distance':tv_base,'method':'baseline'})\n",
    "    new_row_bary=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_bary),\n",
    "                        'f1 macro':f1_score(df_test['Y'], y_pred_bary, average='macro',sample_weight=df_test['W']),\n",
    "                        'f1 micro':f1_score(df_test['Y'], y_pred_bary, average='micro',sample_weight=df_test['W']),\n",
    "                        'f1 weighted':f1_score(df_test['Y'], y_pred_bary, average='weighted',sample_weight=df_test['W']),\n",
    "                        'TV distance':tv_bary,'method':'barycentre'})\n",
    "    new_row_part2=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_part2),\n",
    "                        'f1 macro':f1_score(df_test['Y'], y_pred_part2, average='macro',sample_weight=df_test['W']),\n",
    "                        'f1 micro':f1_score(df_test['Y'], y_pred_part2, average='micro',sample_weight=df_test['W']),\n",
    "                        'f1 weighted':f1_score(df_test['Y'], y_pred_part2, average='weighted',sample_weight=df_test['W']),\n",
    "                        'TV distance':tv_part2,'method':'partial repair2'})\n",
    "    # new_row_part3=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_part3),\n",
    "    #                     'f1 macro':f1_score(df_test['Y'], y_pred_part3, average='macro',sample_weight=df_test['W']),\n",
    "    #                     'f1 micro':f1_score(df_test['Y'], y_pred_part3, average='micro',sample_weight=df_test['W']),\n",
    "    #                     'f1 weighted':f1_score(df_test['Y'], y_pred_part3, average='weighted',sample_weight=df_test['W']),\n",
    "    #                     'TV distance':tv_part3,'method':'partial repair3'})\n",
    "    # new_row_total=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_total),\n",
    "    #                     'f1 macro':f1_score(df_test['Y'], y_pred_total, average='macro',sample_weight=df_test['W']),\n",
    "    #                     'f1 micro':f1_score(df_test['Y'], y_pred_total, average='micro',sample_weight=df_test['W']),\n",
    "    #                     'f1 weighted':f1_score(df_test['Y'], y_pred_total, average='weighted',sample_weight=df_test['W']),\n",
    "    #                     'TV distance':tv_total,'method':'total repair'})\n",
    "    # new_row_RW=pd.Series({'DI':DisparateImpact_postprocess(test_RW,y_pred_RW),\n",
    "    #                     'f1 macro':f1_score(test_RW['Y'], y_pred_RW, average='macro',sample_weight=test_RW['W']),\n",
    "    #                     'f1 micro':f1_score(test_RW['Y'], y_pred_RW, average='micro',sample_weight=test_RW['W']),\n",
    "    #                     'f1 weighted':f1_score(test_RW['Y'], y_pred_RW, average='weighted',sample_weight=test_RW['W']),\n",
    "    #                     'TV distance':tv_RW,'method':'reweighting'})\n",
    "    \n",
    "    report = pd.concat([report,new_row.to_frame().T,new_row_base.to_frame().T,new_row_bary.to_frame().T,new_row_part2.to_frame().T], ignore_index=True)\n",
    "    #report = pd.concat([report,new_row.to_frame().T,new_row_base.to_frame().T,new_row_part2.to_frame().T,new_row_part3.to_frame().T,new_row_part4.to_frame().T], ignore_index=True) #,new_row_part4.to_frame().T\n",
    "    # report = pd.concat([report,new_row.to_frame().T,new_row_base.to_frame().T,new_row_bary.to_frame().T,new_row_part3.to_frame().T,new_row_total.to_frame().T], ignore_index=True) #new_row_part2.to_frame().T,\n",
    "    #report = pd.concat([report,new_row.to_frame().T,new_row_base.to_frame().T,new_row_bary.to_frame().T,new_row_part2.to_frame().T,new_row_part3.to_frame().T,new_row_part4.to_frame().T], ignore_index=True) #,new_row_part4.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1  0.19 0.28 0.37 0.46 0.55 0.64 0.73 0.82 0.91]\n"
     ]
    }
   ],
   "source": [
    "num_thresh = 10\n",
    "ba_arr = np.zeros(num_thresh)\n",
    "class_thresh_arr = np.linspace(0.1, 0.91, num_thresh)\n",
    "print(class_thresh_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_part2=partial_repair(C,e,px,ptx,V,1.0e-3,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced accuracy (no fairness constraints) = 0.8144\n",
      "Optimal classification threshold (no fairness constraints) = 0.1900\n"
     ]
    }
   ],
   "source": [
    "for idx, thresh in enumerate(class_thresh_arr):\n",
    "    y_pred_base=postprocess(df_test,coupling_part2,x_list,x_range,var_list,var_range,clf,thresh)\n",
    "    ba_arr[idx] = f1_score(df_test['Y'], y_pred_base, average='micro',sample_weight=df_test['W'])\n",
    "\n",
    "best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "best_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.734539  , 0.8143603 , 0.81366058, 0.81387588, 0.80892405,\n",
       "       0.80865493, 0.8041337 , 0.80477959, 0.80472577, 0.80106572])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DI</th>\n",
       "      <th>f1 macro</th>\n",
       "      <th>f1 micro</th>\n",
       "      <th>f1 weighted</th>\n",
       "      <th>TV distance</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.353593</td>\n",
       "      <td>0.677809</td>\n",
       "      <td>0.81619</td>\n",
       "      <td>0.786812</td>\n",
       "      <td>0.19143</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.353593</td>\n",
       "      <td>0.677809</td>\n",
       "      <td>0.81619</td>\n",
       "      <td>0.786812</td>\n",
       "      <td>0.191351</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.73633</td>\n",
       "      <td>0.535094</td>\n",
       "      <td>0.702298</td>\n",
       "      <td>0.679023</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982744</td>\n",
       "      <td>0.627093</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>0.716284</td>\n",
       "      <td>0.025413</td>\n",
       "      <td>partial repair2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.679819</td>\n",
       "      <td>0.675185</td>\n",
       "      <td>0.814967</td>\n",
       "      <td>0.785183</td>\n",
       "      <td>0.150419</td>\n",
       "      <td>reweighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.44555</td>\n",
       "      <td>0.690283</td>\n",
       "      <td>0.823241</td>\n",
       "      <td>0.797115</td>\n",
       "      <td>0.195836</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.44555</td>\n",
       "      <td>0.690283</td>\n",
       "      <td>0.823241</td>\n",
       "      <td>0.797115</td>\n",
       "      <td>0.195673</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.931475</td>\n",
       "      <td>0.54916</td>\n",
       "      <td>0.701222</td>\n",
       "      <td>0.687002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.872677</td>\n",
       "      <td>0.6561</td>\n",
       "      <td>0.753431</td>\n",
       "      <td>0.752417</td>\n",
       "      <td>0.025543</td>\n",
       "      <td>partial repair2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.77549</td>\n",
       "      <td>0.688621</td>\n",
       "      <td>0.822451</td>\n",
       "      <td>0.79609</td>\n",
       "      <td>0.160632</td>\n",
       "      <td>reweighting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DI  f1 macro  f1 micro f1 weighted TV distance           method\n",
       "0  0.353593  0.677809   0.81619    0.786812     0.19143           origin\n",
       "1  0.353593  0.677809   0.81619    0.786812    0.191351         baseline\n",
       "2   0.73633  0.535094  0.702298    0.679023    0.000049       barycentre\n",
       "3  0.982744  0.627093  0.707142    0.716284    0.025413  partial repair2\n",
       "4  0.679819  0.675185  0.814967    0.785183    0.150419      reweighting\n",
       "5   0.44555  0.690283  0.823241    0.797115    0.195836           origin\n",
       "6   0.44555  0.690283  0.823241    0.797115    0.195673         baseline\n",
       "7  0.931475   0.54916  0.701222    0.687002     0.00001       barycentre\n",
       "8  0.872677    0.6561  0.753431    0.752417    0.025543  partial repair2\n",
       "9   0.77549  0.688621  0.822451     0.79609    0.160632      reweighting"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DI</th>\n",
       "      <th>f1 macro</th>\n",
       "      <th>f1 micro</th>\n",
       "      <th>f1 weighted</th>\n",
       "      <th>TV distance</th>\n",
       "      <th>method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.449827</td>\n",
       "      <td>0.678789</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>0.789005</td>\n",
       "      <td>0.205545</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.449827</td>\n",
       "      <td>0.678789</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>0.789005</td>\n",
       "      <td>0.205545</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.04678</td>\n",
       "      <td>0.547657</td>\n",
       "      <td>0.641296</td>\n",
       "      <td>0.654991</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75701</td>\n",
       "      <td>0.658003</td>\n",
       "      <td>0.786661</td>\n",
       "      <td>0.7674</td>\n",
       "      <td>0.012038</td>\n",
       "      <td>partial repair2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.895749</td>\n",
       "      <td>0.677025</td>\n",
       "      <td>0.816853</td>\n",
       "      <td>0.787855</td>\n",
       "      <td>0.16003</td>\n",
       "      <td>reweighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.405739</td>\n",
       "      <td>0.678773</td>\n",
       "      <td>0.817219</td>\n",
       "      <td>0.787287</td>\n",
       "      <td>0.210459</td>\n",
       "      <td>origin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.405739</td>\n",
       "      <td>0.678773</td>\n",
       "      <td>0.817219</td>\n",
       "      <td>0.787287</td>\n",
       "      <td>0.210459</td>\n",
       "      <td>baseline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.766167</td>\n",
       "      <td>0.538426</td>\n",
       "      <td>0.652301</td>\n",
       "      <td>0.656396</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>barycentre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.757408</td>\n",
       "      <td>0.658621</td>\n",
       "      <td>0.786661</td>\n",
       "      <td>0.7662</td>\n",
       "      <td>0.012319</td>\n",
       "      <td>partial repair2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.843715</td>\n",
       "      <td>0.674337</td>\n",
       "      <td>0.815176</td>\n",
       "      <td>0.784537</td>\n",
       "      <td>0.164602</td>\n",
       "      <td>reweighting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DI  f1 macro  f1 micro f1 weighted TV distance           method\n",
       "0  0.449827  0.678789  0.817833    0.789005    0.205545           origin\n",
       "1  0.449827  0.678789  0.817833    0.789005    0.205545         baseline\n",
       "2   1.04678  0.547657  0.641296    0.654991    0.000075       barycentre\n",
       "3   0.75701  0.658003  0.786661      0.7674    0.012038  partial repair2\n",
       "4  0.895749  0.677025  0.816853    0.787855     0.16003      reweighting\n",
       "5  0.405739  0.678773  0.817219    0.787287    0.210459           origin\n",
       "6  0.405739  0.678773  0.817219    0.787287    0.210459         baseline\n",
       "7  0.766167  0.538426  0.652301    0.656396    0.001311       barycentre\n",
       "8  0.757408  0.658621  0.786661      0.7662    0.012319  partial repair2\n",
       "9  0.843715  0.674337  0.815176    0.784537    0.164602      reweighting"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_csv(path+'/data/report_postprocess_bary'+str(pa)+'.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
