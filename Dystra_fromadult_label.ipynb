{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "def normialise(tem_dist):\n",
    "    return [tem_dist[i]/sum(tem_dist) for i in range(len(tem_dist))]\n",
    "\n",
    "def tmp_generator(gamma_dict,num,q_dict,q_num,L):\n",
    "    bin=gamma_dict[0].shape[0]\n",
    "    if q_num<=0:\n",
    "        q=np.matrix(np.ones((bin,bin)))\n",
    "    else:\n",
    "        q=q_dict[q_num]\n",
    "    tmp_gamma=np.zeros((bin,bin))\n",
    "    tmp_q=np.zeros((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            if gamma_dict[num-L].item(i,j) != 0:\n",
    "                tmp_gamma[i,j]=q.item(i,j)*gamma_dict[num-1].item(i,j)*gamma_dict[num-L-1].item(i,j)/gamma_dict[num-L].item(i,j)\n",
    "                tmp_q[i,j]=q.item(i,j)*gamma_dict[num-L-1].item(i,j)/gamma_dict[num-L].item(i,j)\n",
    "            else:\n",
    "                tmp_gamma[i,j]=q.item(i,j)*gamma_dict[num-1].item(i,j)*gamma_dict[num-L-1].item(i,j)/(1.0e-9)\n",
    "                tmp_q[i,j]=q.item(i,j)*gamma_dict[num-L-1].item(i,j)/(1.0e-9)\n",
    "    return np.matrix(tmp_gamma),np.matrix(tmp_q)     \n",
    "\n",
    "def assess(bin,f,g,C,V,output):\n",
    "    output=output.A1.reshape((bin,bin))\n",
    "    print('sum of violation of f:',sum(abs(np.sum(output,1)-f)))\n",
    "    print('sum of violation of g:',sum(abs(np.sum(output,0)-g)))\n",
    "    print('total cost:',sum(sum(output*C)))\n",
    "    print('entropy:',sum(sum(-output*np.log(output+0.1**3))))\n",
    "    print('tr violation:',sum(abs(output.T@V)))\n",
    "    print('============================================')\n",
    "\n",
    "def plots(x_range,g,f,output):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=(4,1), height_ratios=(1,4),left=0.1,right=0.9,bottom=0.1, top=0.9,wspace=0,hspace=0)\n",
    "    # Create the Axes.\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax.pcolormesh(x_range, x_range, output, cmap='Blues')\n",
    "    ax.set_xlabel(r'supp($X$)',fontsize=10)\n",
    "    ax.set_ylabel(r'supp($\\tilde{X}$)',fontsize=10)#\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax) \n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "    #ax_histx.set_title(r'$Pr[x]$',rotation='horizontal')\n",
    "    #ax_histy.set_title(r'$Pr[\\tilde{x}]$')\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histx.tick_params(axis=\"y\", labelleft=False)\n",
    "    ax_histy.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "    ax_histx.plot(x_range,g,color='tab:blue')\n",
    "    ax_histy.plot(f,x_range,color='tab:green') \n",
    "    return fig\n",
    "\n",
    "def v_value(x0,x1,x):\n",
    "    if x != 0:\n",
    "        return (x0-x1)/x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def newton(fun,dfun,a, stepmax, tol):\n",
    "    if abs(fun(a))<=tol: return a\n",
    "    for step in range(1, stepmax+1):\n",
    "        b=a-fun(a)/dfun(a)\n",
    "        if abs(fun(b))<=tol:\n",
    "            return b\n",
    "        else:\n",
    "            a = b\n",
    "    return b \n",
    "\n",
    "# simplist\n",
    "def baseline(C,e,px,ptx,V,K):\n",
    "    bin=len(px)\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    #I=np.where(~(V==0))[0].tolist()\n",
    "    xi=np.exp(-C/e)\n",
    "    gamma_classic=dict()\n",
    "    gamma_classic[0]=np.matrix(xi+1.0e-9)\n",
    "    for repeat in range(K):\n",
    "        gamma_classic[1+2*repeat]=np.matrix(np.diag((px/(gamma_classic[2*repeat] @ bbm1)).A1))@gamma_classic[2*repeat] #np.diag(dist['x']/sum(gamma_classic.T))@gamma_classic\n",
    "        gamma_classic[2+2*repeat]=gamma_classic[1+2*repeat]@np.matrix(np.diag((ptx/(gamma_classic[1+2*repeat].T @ bbm1)).A1))\n",
    "\n",
    "    assess(bin,dist['x'],dist['t_x'],C,V,gamma_classic[2*K])\n",
    "    return gamma_classic\n",
    "\n",
    "# our method | total repair\n",
    "def total_repair(C,e,px,ptx,V,K):\n",
    "    bin=len(px)\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    I=np.where(~(V==0))[0].tolist()\n",
    "    xi=np.exp(-C/e)\n",
    "    gamma_dict=dict()\n",
    "    gamma_dict[0]=np.matrix(xi+1.0e-9)\n",
    "    gamma_dict[1]=np.matrix(np.diag((px/(gamma_dict[0] @ bbm1)).A1))@gamma_dict[0]\n",
    "    gamma_dict[2]=gamma_dict[1]@np.matrix(np.diag((ptx/(gamma_dict[1].T @ bbm1)).A1))\n",
    "    # step 3\n",
    "    J=np.where(~((gamma_dict[2].T @ V).A1 ==0))[0].tolist()\n",
    "    nu=np.zeros(bin)\n",
    "    gamma_dict[3]=np.copy(gamma_dict[2])\n",
    "    for j in J:\n",
    "        fun = lambda z: sum(gamma_dict[2].item(i,j)*V.item(i)*np.exp(z*V.item(i)) for i in I)\n",
    "        dfun = lambda z: sum(gamma_dict[2].item(i,j)*(V.item(i))**2*np.exp(z*V.item(i)) for i in I)\n",
    "        nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-3) #bisection(fun, -50,50, stepmax = 25, tol = 1.0e-3)\n",
    "        for i in I:\n",
    "            gamma_dict[3][i,j]=np.exp(nu*V.item(i))*gamma_dict[2].item(i,j)\n",
    "    gamma_dict[3]=np.matrix(gamma_dict[3])\n",
    "\n",
    "    #=========================\n",
    "    L=3\n",
    "    q_dict=dict()\n",
    "    for loop in range(1,K):\n",
    "        tmp,q_dict[(loop-1)*L+1]=tmp_generator(gamma_dict,loop*L+1,q_dict,(loop-2)*L+1,L) #np.matrix(gamma_dict[3].A1*gamma_dict[0].A1/gamma_dict[1].A1)\n",
    "        gamma_dict[loop*L+1]=np.matrix(np.diag((px/(tmp @ bbm1)).A1))@tmp\n",
    "\n",
    "        tmp,q_dict[(loop-1)*L+2]=tmp_generator(gamma_dict,loop*L+2,q_dict,(loop-2)*L+2,L)  #np.matrix(gamma_dict[4].A1*gamma_dict[1].A1/gamma_dict[2].A1)\n",
    "        gamma_dict[loop*L+2]=tmp@np.matrix(np.diag((ptx/(tmp.T @ bbm1)).A1))\n",
    "\n",
    "        # step 3\n",
    "        tmp,q_dict[(loop-1)*L+3]=tmp_generator(gamma_dict,loop*L+3,q_dict,(loop-2)*L+3,L)  #np.matrix(gamma_dict[5].A1*gamma_dict[2].A1/gamma_dict[3].A1)\n",
    "        J=np.where(~((abs(np.matrix(tmp).T @ V).A1)<=1.0e-5))[0].tolist()\n",
    "        gamma_dict[loop*L+3]=np.copy(tmp)\n",
    "        for j in J:\n",
    "            fun = lambda z: sum(tmp.item(i,j)*V.item(i)*np.exp(z*V.item(i)) for i in I)\n",
    "            dfun = lambda z: sum(tmp.item(i,j)*(V.item(i))**2*np.exp(z*V.item(i)) for i in I)\n",
    "            nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-5) \n",
    "            for i in I:\n",
    "                gamma_dict[loop*L+3][i,j]=np.exp(nu*V.item(i))*tmp.item(i,j)\n",
    "        gamma_dict[loop*L+3]=np.matrix(gamma_dict[loop*L+3])\n",
    "\n",
    "    assess(bin,dist['x'],dist['t_x'],C,V,gamma_dict[K*L])\n",
    "    return gamma_dict\n",
    "\n",
    "# our method | partial repair\n",
    "def partial_repair(C,e,px,ptx,V,theta_scale,K):\n",
    "    bin=len(px)\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    I=np.where(~(V==0))[0].tolist()\n",
    "    xi=np.exp(-C/e)\n",
    "    theta=bbm1*theta_scale\n",
    "    gamma_dict=dict()\n",
    "    gamma_dict[0]=np.matrix(xi+1.0e-9)\n",
    "    gamma_dict[1]=np.matrix(np.diag((px/(gamma_dict[0] @ bbm1)).A1))@gamma_dict[0]\n",
    "    gamma_dict[2]=gamma_dict[1]@np.matrix(np.diag((ptx/(gamma_dict[1].T @ bbm1)).A1))\n",
    "    # step 3\n",
    "    Jplus=np.where(~((gamma_dict[2].T @ V).A1 <=theta.A1))[0].tolist()\n",
    "    Jminus=np.where(~((gamma_dict[2].T @ V).A1>=-theta.A1))[0].tolist()\n",
    "    gamma_dict[3]=np.copy(gamma_dict[2])\n",
    "    for j in Jplus:\n",
    "        fun = lambda z: sum(gamma_dict[2].item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)-theta.item(j)\n",
    "        dfun = lambda z: -sum(gamma_dict[2].item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "        nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-3) #bisection(fun, -50,50, stepmax = 25, tol = 1.0e-3)\n",
    "        for i in I:\n",
    "            gamma_dict[3][i,j]=np.exp(-nu*V.item(i))*gamma_dict[2].item(i,j)\n",
    "    for j in Jminus:\n",
    "        fun = lambda z: sum(gamma_dict[2].item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)+theta.item(j)\n",
    "        dfun = lambda z: -sum(gamma_dict[2].item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "        nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-3) #bisection(fun, -50,50, stepmax = 25, tol = 1.0e-3)\n",
    "        for i in I:\n",
    "            gamma_dict[3][i,j]=np.exp(-nu*V.item(i))*gamma_dict[2].item(i,j)\n",
    "    gamma_dict[3]=np.matrix(gamma_dict[3])\n",
    "\n",
    "    #=========================\n",
    "    L=3\n",
    "    q_dict=dict()\n",
    "    for loop in range(1,K):\n",
    "        tmp,q_dict[(loop-1)*L+1]=tmp_generator(gamma_dict,loop*L+1,q_dict,(loop-2)*L+1,L) #np.matrix(gamma_dict[3].A1*gamma_dict[0].A1/gamma_dict[1].A1)\n",
    "        gamma_dict[loop*L+1]=np.matrix(np.diag((px/(tmp @ bbm1)).A1))@tmp\n",
    "\n",
    "        tmp,q_dict[(loop-1)*L+2]=tmp_generator(gamma_dict,loop*L+2,q_dict,(loop-2)*L+2,L)  #np.matrix(gamma_dict[4].A1*gamma_dict[1].A1/gamma_dict[2].A1)\n",
    "        gamma_dict[loop*L+2]=tmp@np.matrix(np.diag((ptx/(tmp.T @ bbm1)).A1))\n",
    "\n",
    "        # step 3\n",
    "        tmp,q_dict[(loop-1)*L+3]=tmp_generator(gamma_dict,loop*L+3,q_dict,(loop-2)*L+3,L)  #np.matrix(gamma_dict[5].A1*gamma_dict[2].A1/gamma_dict[3].A1)\n",
    "        Jplus=np.where(~((np.matrix(tmp).T @ V).A1 <=theta.A1))[0].tolist()\n",
    "        Jminus=np.where(~((np.matrix(tmp).T @ V).A1>=-theta.A1))[0].tolist()\n",
    "        gamma_dict[loop*L+3]=np.copy(tmp)\n",
    "        for j in Jplus:\n",
    "            fun = lambda z: sum(tmp.item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)-theta.item(j)\n",
    "            dfun = lambda z: -sum(tmp.item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "            nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-5) \n",
    "            for i in I:\n",
    "                gamma_dict[loop*L+3][i,j]=np.exp(-nu*V.item(i))*tmp.item(i,j)\n",
    "        for j in Jminus:\n",
    "            fun = lambda z: sum(tmp.item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)+theta.item(j)\n",
    "            dfun = lambda z: -sum(tmp.item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "            nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-5) \n",
    "            for i in I:\n",
    "                gamma_dict[loop*L+3][i,j]=np.exp(-nu*V.item(i))*tmp.item(i,j)\n",
    "        gamma_dict[loop*L+3]=np.matrix(gamma_dict[loop*L+3])\n",
    "\n",
    "    assess(bin,dist['x'],dist['t_x'],C,V,gamma_dict[L*K])\n",
    "    return gamma_dict\n",
    "\n",
    "def empirical_distribution(sub,x_range):\n",
    "    bin=len(x_range)\n",
    "    distrition=np.zeros(bin)\n",
    "    for i in range(bin):\n",
    "        subset=sub[sub['X']==x_range[i]] #bin_value=x_range[i] #sub[(sub['X']>=bin_value)&(sub['X']<bin_value+width)]\n",
    "        if subset.shape[0]>0:\n",
    "            distrition[i]=sum(subset['W'])\n",
    "    if sum(distrition)>0:\n",
    "        return distrition/sum(distrition)\n",
    "    else:\n",
    "        return distrition\n",
    "\n",
    "def plot_rdist(rdist,x_range):\n",
    "    plt.plot(x_range,rdist['x'],label=r'$Pr[x]$',color='tab:blue')\n",
    "    plt.plot(x_range,rdist['x_0'],label=r'$Pr[x|s_0]$',alpha=0.3,color='tab:orange')\n",
    "    plt.plot(x_range,rdist['x_1'],label=r'$Pr[x|s_1]$',alpha=0.3,color='#9f86c0')\n",
    "    plt.ylabel('PMF',fontsize=14)\n",
    "    plt.xlabel(r'$supp(X)=supp(\\tilde{X})$',fontsize=20)\n",
    "    plt.legend()\n",
    "    return plt\n",
    "\n",
    "def DisparateImpact(X_test,y_pred):\n",
    "    dim=X_test.shape[1]-2\n",
    "    df_test=pd.DataFrame(np.concatenate((X_test,y_pred.reshape(-1,1)), axis=1),columns=[*range(dim)]+['S','W','f'])\n",
    "    numerator=sum(df_test[(df_test['S']==0)&(df_test['f']==1)]['W'])/sum(df_test[df_test['S']==0]['W'])\n",
    "    denominator=sum(df_test[(df_test['S']==1)&(df_test['f']==1)]['W'])/sum(df_test[df_test['S']==1]['W'])\n",
    "    return numerator/denominator\n",
    "    \n",
    "def rdata_analysis(rdata,x_range,x_name):\n",
    "    rdist=dict()\n",
    "    pivot=pd.pivot_table(rdata,index=x_name,values=['W'],aggfunc=[np.sum])[('sum','W')]\n",
    "    pivot0=pd.pivot_table(rdata[rdata['S']==0],index=x_name,values=['W'],aggfunc=[np.sum])[('sum','W')]\n",
    "    pivot1=pd.pivot_table(rdata[rdata['S']==1],index=x_name,values=['W'],aggfunc=[np.sum])[('sum','W')]\n",
    "    rdist['x']= np.array([pivot[i] for i in x_range])/sum([pivot[i] for i in x_range]) #empirical_distribution(rdata,x_range)\n",
    "    rdist['x_0']=np.array([pivot0[i] if i in list(pivot0.index) else 0 for i in x_range])/sum([pivot0[i] if i in list(pivot0.index) else 0 for i in x_range]) #empirical_distribution(rdata[rdata['S']==0],x_range)\n",
    "    rdist['x_1']=np.array([pivot1[i] if i in list(pivot1.index) else 0 for i in x_range])/sum([pivot1[i] if i in list(pivot1.index) else 0 for i in x_range]) #empirical_distribution(rdata[rdata['S']==1],x_range)\n",
    "    return rdist\n",
    "\n",
    "def c_generate(x_range):\n",
    "    bin=len(x_range)\n",
    "    C=np.random.random((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            C[i,j]=abs(x_range[i]-x_range[j]) \n",
    "    return C\n",
    "\n",
    "def c_generate_higher(x_range,weight):\n",
    "    bin=len(x_range)\n",
    "    dim=len(x_range[0])\n",
    "    C=np.random.random((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            C[i,j]=sum(weight[d]*abs(x_range[i][d]-x_range[j][d]) for d in range(dim))\n",
    "    return C\n",
    "\n",
    "def projection(df,coupling_matrix,x_range,x_name,var_list):\n",
    "    bin=len(x_range)\n",
    "    var_list_tmp=var_list[:]\n",
    "    var_list_tmp.remove(x_name)\n",
    "    var_list_tmp=[x_name]+var_list_tmp # place the var that needs to be repaired the first\n",
    "    df=df[var_list_tmp+['S','W','Y']]\n",
    "    coupling=coupling_matrix.A1.reshape((bin,bin))\n",
    "    df_t=pd.DataFrame(columns=var_list_tmp+['S','W','Y'])\n",
    "    for i in range(df.shape[0]):\n",
    "        orig=df.iloc[i]\n",
    "        loc=np.where([x_range[i]==orig[x_name] for i in range(bin)])[0][0]\n",
    "        rows=np.nonzero(coupling[loc,:])[0]\n",
    "        sub_dict={x_name:[x_range[r] for r in rows],'W':list(coupling[loc,rows]/(sum(coupling[loc,rows]))*orig['W'])}\n",
    "        sub_dict.update({var:[orig[var]]*len(rows) for var in var_list_tmp[1:]+['S','Y']})\n",
    "        sub=pd.DataFrame(data=sub_dict, index=rows)\n",
    "        df_t=pd.concat([df_t,sub],ignore_index=True)#pd.concat([df_t,samples_groupby(sub,x_list)], ignore_index=True)\n",
    "    df_t=df_t.groupby(by=list(chain(*[var_list,'S','Y'])),as_index=False).sum()\n",
    "    df_t=df_t[var_list+['S','W','Y']]\n",
    "    return df_t\n",
    "\n",
    "def projection_higher(df,coupling_matrix,x_range,x_list,var_list):\n",
    "    df=df.drop(columns=x_list)\n",
    "    bin=len(x_range)\n",
    "    arg_list=[elem for elem in var_list if elem not in x_list]\n",
    "    df=df[arg_list+['X','S','W','Y']]\n",
    "    coupling=coupling_matrix.A1.reshape((bin,bin))\n",
    "    df_t=pd.DataFrame(columns=arg_list+['X','S','W','Y'])\n",
    "    for i in range(df.shape[0]):\n",
    "        orig=df.iloc[i]\n",
    "        loc=np.where([x_range[i]==orig['X'] for i in range(bin)])[0][0]\n",
    "        rows=np.nonzero(coupling[loc,:])[0]\n",
    "        sub_dict={'X':[x_range[r] for r in rows],'W':list(coupling[loc,rows]/(sum(coupling[loc,rows]))*orig['W'])}\n",
    "        sub_dict.update({var:[orig[var]]*len(rows) for var in arg_list+['S','Y']})\n",
    "        sub=pd.DataFrame(data=sub_dict, index=rows)\n",
    "        df_t=pd.concat([df_t,sub],ignore_index=True)#pd.concat([df_t,samples_groupby(sub,x_list)], ignore_index=True)\n",
    "    df_t=df_t.groupby(by=list(chain(*[arg_list,'X','S','Y'])),as_index=False).sum()\n",
    "    for d in range(dim):\n",
    "        df_t[x_list[d]]=[df_t['X'][r][d] for r in range(df_t.shape[0])]\n",
    "    return df_t[var_list+['S','W','Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m tv_dist\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m()\n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m x_name \u001b[39min\u001b[39;00m var_list:\n\u001b[1;32m---> 21\u001b[0m     x_range\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(pd\u001b[39m.\u001b[39;49mpivot_table(messydata,index\u001b[39m=\u001b[39;49mx_name,values\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mW\u001b[39;49m\u001b[39m'\u001b[39;49m])[(\u001b[39m'\u001b[39m\u001b[39mW\u001b[39m\u001b[39m'\u001b[39m)]\u001b[39m.\u001b[39mindex) \n\u001b[0;32m     22\u001b[0m     dist\u001b[39m=\u001b[39mrdata_analysis(messydata,x_range,x_name)\n\u001b[0;32m     23\u001b[0m     tv_dist[x_name]\u001b[39m=\u001b[39m\u001b[39msum\u001b[39m(\u001b[39mabs\u001b[39m(dist[\u001b[39m'\u001b[39m\u001b[39mx_0\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m-\u001b[39mdist[\u001b[39m'\u001b[39m\u001b[39mx_1\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\zhouq\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:97\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     94\u001b[0m     table \u001b[39m=\u001b[39m concat(pieces, keys\u001b[39m=\u001b[39mkeys, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m table \u001b[39m=\u001b[39m __internal_pivot_table(\n\u001b[0;32m     98\u001b[0m     data,\n\u001b[0;32m     99\u001b[0m     values,\n\u001b[0;32m    100\u001b[0m     index,\n\u001b[0;32m    101\u001b[0m     columns,\n\u001b[0;32m    102\u001b[0m     aggfunc,\n\u001b[0;32m    103\u001b[0m     fill_value,\n\u001b[0;32m    104\u001b[0m     margins,\n\u001b[0;32m    105\u001b[0m     dropna,\n\u001b[0;32m    106\u001b[0m     margins_name,\n\u001b[0;32m    107\u001b[0m     observed,\n\u001b[0;32m    108\u001b[0m     sort,\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m table\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpivot_table\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zhouq\\anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py:166\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(values)\n\u001b[1;32m--> 166\u001b[0m grouped \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgroupby(keys, observed\u001b[39m=\u001b[39;49mobserved, sort\u001b[39m=\u001b[39;49msort)\n\u001b[0;32m    167\u001b[0m msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    168\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpivot_table dropped a column because it failed to aggregate. This behavior \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis deprecated and will raise in a future version of pandas. Select only the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns that can be aggregated.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m )\n\u001b[0;32m    172\u001b[0m \u001b[39mwith\u001b[39;00m rewrite_warning(\n\u001b[0;32m    173\u001b[0m     target_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe default value of numeric_only\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    174\u001b[0m     target_category\u001b[39m=\u001b[39m\u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m     new_message\u001b[39m=\u001b[39mmsg,\n\u001b[0;32m    176\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\zhouq\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to supply one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mby\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlevel\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[39m=\u001b[39;49mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[39m=\u001b[39;49mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[39m=\u001b[39;49mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[39m=\u001b[39;49mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zhouq\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[39mif\u001b[39;00m grouper \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgroupby\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrouper\u001b[39;00m \u001b[39mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[39m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[39m=\u001b[39;49mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mc:\\Users\\zhouq\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, gpr, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(gpr, Grouper) \u001b[39mand\u001b[39;00m gpr\u001b[39m.\u001b[39mkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[39m.\u001b[39madd(gpr\u001b[39m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "K=150\n",
    "e=0.01\n",
    "var_list=['age','education-num','capitalgain','capitalloss','hoursperweek','class']\n",
    "var_dim=len(var_list)\n",
    "messydata=pd.read_csv('C:/Users/zhouq/Documents/optimal_transport/adult_csv.csv',usecols=var_list+['race'])\n",
    "#S=1: White S=0: Black\n",
    "messydata=messydata[(messydata['race']=='White')|(messydata['race']=='Black')]\n",
    "messydata['race']=messydata['race'].replace('White',1)\n",
    "messydata['race']=messydata['race'].replace('Black',0)\n",
    "messydata['class']=messydata['class'].replace('>50K',1)\n",
    "messydata['class']=messydata['class'].replace('<=50K',0)\n",
    "for col in ['race','class']+var_list:\n",
    "    messydata[col]=messydata[col].astype('category')\n",
    "#cat_columns = messydata.select_dtypes(['category']).columns\n",
    "#messydata[cat_columns]=messydata[cat_columns].apply(lambda x: x.cat.codes)\n",
    "messydata=messydata.rename(columns={'race':'S','class':'Y'}) #'education-num':'X1','hoursperweek':'X2',\n",
    "messydata['W']=1\n",
    "\n",
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range,x_name)\n",
    "    tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
