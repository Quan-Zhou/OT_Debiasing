{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def c_generate(x_range):\n",
    "    bin=len(x_range)\n",
    "    C=np.random.random((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            C[i,j]=abs(x_range[i]-x_range[j])\n",
    "    return C\n",
    "\n",
    "def normialise(tem_dist):\n",
    "    return [tem_dist[i]/sum(tem_dist) for i in range(len(tem_dist))]\n",
    "\n",
    "def tmp_generator(gamma_dict,num,q_dict,q_num,L):\n",
    "    bin=gamma_dict[0].shape[0]\n",
    "    if q_num<=0:\n",
    "        q=np.matrix(np.ones((bin,bin)))\n",
    "    else:\n",
    "        q=q_dict[q_num]\n",
    "    tmp_gamma=np.zeros((bin,bin))\n",
    "    tmp_q=np.zeros((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            tmp_gamma[i,j]=q.item(i,j)*gamma_dict[num-1].item(i,j)*gamma_dict[num-L-1].item(i,j)/gamma_dict[num-L].item(i,j)\n",
    "            tmp_q[i,j]=q.item(i,j)*gamma_dict[num-L-1].item(i,j)/gamma_dict[num-L].item(i,j)\n",
    "    return np.matrix(tmp_gamma),np.matrix(tmp_q)     \n",
    "\n",
    "def assess(bin,f,g,C,V,output):\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    print('sum of violation of f:',sum(abs(output*bbm1-f)))\n",
    "    print('sum of violation of g:',sum(abs(output.T*bbm1-g)))\n",
    "    # print('sum of violation of f:',sum(abs(np.sum(output,1)-np.array(f))))\n",
    "    # print('sum of violation of g:',sum(abs(np.sum(output,0)-np.array(g))))\n",
    "    output=output.A1.reshape((bin,bin))\n",
    "    print('total cost:',sum(sum(output*C)))\n",
    "    print('entropy:',sum(sum(-output*np.log(output+0.1**3))))\n",
    "    print('tr violation:',sum(abs(output.T@V)))\n",
    "    print('============================================')\n",
    "\n",
    "def plots(x_range,g,f,output):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    gs = fig.add_gridspec(2, 2, width_ratios=(4,1), height_ratios=(1,4),left=0.1,right=0.9,bottom=0.1, top=0.9,wspace=0,hspace=0)\n",
    "    # Create the Axes.\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax.pcolormesh(x_range, x_range, output, cmap='Blues')\n",
    "    ax.set_xlabel(r'supp($X$)',fontsize=10)\n",
    "    ax.set_ylabel(r'supp($\\tilde{X}$)',fontsize=10)#\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax) \n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "    #ax_histx.set_title(r'$Pr[x]$',rotation='horizontal')\n",
    "    #ax_histy.set_title(r'$Pr[\\tilde{x}]$')\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histx.tick_params(axis=\"y\", labelleft=False)\n",
    "    ax_histy.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "    ax_histx.plot(x_range,g,color='tab:blue')\n",
    "    ax_histy.plot(f,x_range,color='tab:green') \n",
    "    return fig\n",
    "    \n",
    "def newton(fun,dfun,a, stepmax, tol):\n",
    "    if abs(fun(a))<=tol: return a\n",
    "    for step in range(1, stepmax+1):\n",
    "        b=a-fun(a)/dfun(a)\n",
    "        if abs(fun(b))<=tol:\n",
    "            return b\n",
    "        else:\n",
    "            a = b\n",
    "    return b \n",
    "\n",
    "# simplist\n",
    "def baseline(C,e,px,ptx,V,K):\n",
    "    # V is only used for assessment\n",
    "    bin=len(px)\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    #I=np.where(~(V==0))[0].tolist()\n",
    "    xi=np.exp(-C/e)\n",
    "    gamma_classic=dict()\n",
    "    gamma_classic[0]=np.matrix(xi+1.0e-9)\n",
    "    for repeat in range(K):\n",
    "        gamma_classic[1+2*repeat]=np.matrix(np.diag((px/(gamma_classic[2*repeat] @ bbm1)).A1))@gamma_classic[2*repeat] #np.diag(dist['x']/sum(gamma_classic.T))@gamma_classic\n",
    "        gamma_classic[2+2*repeat]=gamma_classic[1+2*repeat]@np.matrix(np.diag((ptx/(gamma_classic[1+2*repeat].T @ bbm1)).A1))\n",
    "\n",
    "    assess(bin,px,ptx,C,V,gamma_classic[2*K])\n",
    "    return gamma_classic\n",
    "\n",
    "# our method | total repair\n",
    "def total_repair(C,e,px,ptx,V,K):\n",
    "    bin=len(px)\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    I=np.where(~(V==0))[0].tolist()\n",
    "    xi=np.exp(-C/e)\n",
    "    gamma_dict=dict()\n",
    "    gamma_dict[0]=np.matrix(xi+1.0e-9)\n",
    "    gamma_dict[1]=np.matrix(np.diag((px/(gamma_dict[0] @ bbm1)).A1))@gamma_dict[0]\n",
    "    gamma_dict[2]=gamma_dict[1]@np.matrix(np.diag((ptx/(gamma_dict[1].T @ bbm1)).A1))\n",
    "    # step 3\n",
    "    J=np.where(~((gamma_dict[2].T @ V).A1 ==0))[0].tolist()\n",
    "    nu=np.zeros(bin)\n",
    "    gamma_dict[3]=np.copy(gamma_dict[2])\n",
    "    for j in J:\n",
    "        fun = lambda z: sum(gamma_dict[2].item(i,j)*V.item(i)*np.exp(z*V.item(i)) for i in I)\n",
    "        dfun = lambda z: sum(gamma_dict[2].item(i,j)*(V.item(i))**2*np.exp(z*V.item(i)) for i in I)\n",
    "        nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-3) #bisection(fun, -50,50, stepmax = 25, tol = 1.0e-3)\n",
    "        for i in I:\n",
    "            gamma_dict[3][i,j]=np.exp(nu*V.item(i))*gamma_dict[2].item(i,j)\n",
    "    gamma_dict[3]=np.matrix(gamma_dict[3])\n",
    "\n",
    "    #=========================\n",
    "    L=3\n",
    "    q_dict=dict()\n",
    "    for loop in range(1,K):\n",
    "        tmp,q_dict[(loop-1)*L+1]=tmp_generator(gamma_dict,loop*L+1,q_dict,(loop-2)*L+1,L) #np.matrix(gamma_dict[3].A1*gamma_dict[0].A1/gamma_dict[1].A1)\n",
    "        gamma_dict[loop*L+1]=np.matrix(np.diag((px/(tmp @ bbm1)).A1))@tmp\n",
    "\n",
    "        tmp,q_dict[(loop-1)*L+2]=tmp_generator(gamma_dict,loop*L+2,q_dict,(loop-2)*L+2,L)  #np.matrix(gamma_dict[4].A1*gamma_dict[1].A1/gamma_dict[2].A1)\n",
    "        gamma_dict[loop*L+2]=tmp@np.matrix(np.diag((ptx/(tmp.T @ bbm1)).A1))\n",
    "\n",
    "        # step 3\n",
    "        tmp,q_dict[(loop-1)*L+3]=tmp_generator(gamma_dict,loop*L+3,q_dict,(loop-2)*L+3,L)  #np.matrix(gamma_dict[5].A1*gamma_dict[2].A1/gamma_dict[3].A1)\n",
    "        J=np.where(~((abs(np.matrix(tmp).T @ V).A1)<=1.0e-5))[0].tolist()\n",
    "        gamma_dict[loop*L+3]=np.copy(tmp)\n",
    "        for j in J:\n",
    "            fun = lambda z: sum(tmp.item(i,j)*V.item(i)*np.exp(z*V.item(i)) for i in I)\n",
    "            dfun = lambda z: sum(tmp.item(i,j)*(V.item(i))**2*np.exp(z*V.item(i)) for i in I)\n",
    "            nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-5) \n",
    "            for i in I:\n",
    "                gamma_dict[loop*L+3][i,j]=np.exp(nu*V.item(i))*tmp.item(i,j)\n",
    "        gamma_dict[loop*L+3]=np.matrix(gamma_dict[loop*L+3])\n",
    "\n",
    "    assess(bin,px,ptx,C,V,gamma_dict[K*L])\n",
    "    return gamma_dict\n",
    "\n",
    "# our method | partial repair\n",
    "def partial_repair(C,e,px,ptx,V,theta_scale,K):\n",
    "    bin=len(px)\n",
    "    bbm1=np.matrix(np.ones(bin)).T\n",
    "    I=np.where(~(V==0))[0].tolist()\n",
    "    xi=np.exp(-C/e)\n",
    "    theta=bbm1*theta_scale\n",
    "    gamma_dict=dict()\n",
    "    gamma_dict[0]=np.matrix(xi+1.0e-9)\n",
    "    gamma_dict[1]=np.matrix(np.diag((px/(gamma_dict[0] @ bbm1)).A1))@gamma_dict[0]\n",
    "    gamma_dict[2]=gamma_dict[1]@np.matrix(np.diag((ptx/(gamma_dict[1].T @ bbm1)).A1))\n",
    "    # step 3\n",
    "    Jplus=np.where(~((gamma_dict[2].T @ V).A1 <=theta.A1))[0].tolist()\n",
    "    Jminus=np.where(~((gamma_dict[2].T @ V).A1>=-theta.A1))[0].tolist()\n",
    "    gamma_dict[3]=np.copy(gamma_dict[2])\n",
    "    for j in Jplus:\n",
    "        fun = lambda z: sum(gamma_dict[2].item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)-theta.item(j)\n",
    "        dfun = lambda z: -sum(gamma_dict[2].item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "        nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-3) #bisection(fun, -50,50, stepmax = 25, tol = 1.0e-3)\n",
    "        for i in I:\n",
    "            gamma_dict[3][i,j]=np.exp(-nu*V.item(i))*gamma_dict[2].item(i,j)\n",
    "    for j in Jminus:\n",
    "        fun = lambda z: sum(gamma_dict[2].item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)+theta.item(j)\n",
    "        dfun = lambda z: -sum(gamma_dict[2].item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "        nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-3) #bisection(fun, -50,50, stepmax = 25, tol = 1.0e-3)\n",
    "        for i in I:\n",
    "            gamma_dict[3][i,j]=np.exp(-nu*V.item(i))*gamma_dict[2].item(i,j)\n",
    "    gamma_dict[3]=np.matrix(gamma_dict[3])\n",
    "\n",
    "    #=========================\n",
    "    L=3\n",
    "    q_dict=dict()\n",
    "    for loop in range(1,K):\n",
    "        tmp,q_dict[(loop-1)*L+1]=tmp_generator(gamma_dict,loop*L+1,q_dict,(loop-2)*L+1,L) #np.matrix(gamma_dict[3].A1*gamma_dict[0].A1/gamma_dict[1].A1)\n",
    "        gamma_dict[loop*L+1]=np.matrix(np.diag((px/(tmp @ bbm1)).A1))@tmp\n",
    "\n",
    "        tmp,q_dict[(loop-1)*L+2]=tmp_generator(gamma_dict,loop*L+2,q_dict,(loop-2)*L+2,L)  #np.matrix(gamma_dict[4].A1*gamma_dict[1].A1/gamma_dict[2].A1)\n",
    "        gamma_dict[loop*L+2]=tmp@np.matrix(np.diag((ptx/(tmp.T @ bbm1)).A1))\n",
    "\n",
    "        # step 3\n",
    "        tmp,q_dict[(loop-1)*L+3]=tmp_generator(gamma_dict,loop*L+3,q_dict,(loop-2)*L+3,L)  #np.matrix(gamma_dict[5].A1*gamma_dict[2].A1/gamma_dict[3].A1)\n",
    "        Jplus=np.where(~((np.matrix(tmp).T @ V).A1 <=theta.A1))[0].tolist()\n",
    "        Jminus=np.where(~((np.matrix(tmp).T @ V).A1>=-theta.A1))[0].tolist()\n",
    "        gamma_dict[loop*L+3]=np.copy(tmp)\n",
    "        for j in Jplus:\n",
    "            fun = lambda z: sum(tmp.item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)-theta.item(j)\n",
    "            dfun = lambda z: -sum(tmp.item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "            nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-5) \n",
    "            for i in I:\n",
    "                gamma_dict[loop*L+3][i,j]=np.exp(-nu*V.item(i))*tmp.item(i,j)\n",
    "        for j in Jminus:\n",
    "            fun = lambda z: sum(tmp.item(i,j)*V.item(i)*np.exp(-z*V.item(i)) for i in I)+theta.item(j)\n",
    "            dfun = lambda z: -sum(tmp.item(i,j)*(V.item(i))**2*np.exp(-z*V.item(i)) for i in I)\n",
    "            nu = newton(fun,dfun,0.5,stepmax = 25,tol = 1.0e-5) \n",
    "            for i in I:\n",
    "                gamma_dict[loop*L+3][i,j]=np.exp(-nu*V.item(i))*tmp.item(i,j)\n",
    "        gamma_dict[loop*L+3]=np.matrix(gamma_dict[loop*L+3])\n",
    "\n",
    "    assess(bin,px,ptx,C,V,gamma_dict[L*K])\n",
    "    return gamma_dict\n",
    "\n",
    "def empirical_distribution(sub,x_range):\n",
    "    bin=len(x_range)\n",
    "    distrition=np.zeros(bin)\n",
    "    for i in range(bin):\n",
    "        subset=sub[sub['X']==x_range[i]] #bin_value=x_range[i] #sub[(sub['X']>=bin_value)&(sub['X']<bin_value+width)]\n",
    "        if subset.shape[0]>0:\n",
    "            distrition[i]=sum(subset['W'])\n",
    "    if sum(distrition)>0:\n",
    "        return distrition/sum(distrition)\n",
    "    else:\n",
    "        return distrition\n",
    "\n",
    "def projection(df,coupling_matrix,x_range):\n",
    "    bin=len(x_range)\n",
    "    coupling=coupling_matrix.A1.reshape((bin,bin))\n",
    "    df_t=pd.DataFrame(columns=['X','S','W','Y'])\n",
    "    for i in range(df.shape[0]):\n",
    "        orig=df.iloc[i]\n",
    "        loc=np.where(x_range==orig[0])[0][0]\n",
    "        rows=np.nonzero(coupling[loc,:])[0]\n",
    "        sub=pd.DataFrame(columns=['X','W'],index=rows)\n",
    "        sub['X']=x_range[rows]\n",
    "        sub['W']=coupling[loc,rows]/(sum(coupling[loc,rows]))*orig[2]\n",
    "        sub['S']=orig[1]\n",
    "        sub['Y']=orig[3]\n",
    "        df_t=pd.concat([df_t, samples_groupby(sub)], ignore_index=True)\n",
    "    return df_t\n",
    "\n",
    "def plot_rdist(rdist,x_range):\n",
    "    plt.plot(x_range,rdist['x'],label=r'$Pr[x]$',color='tab:blue')\n",
    "    plt.plot(x_range,rdist['x_0'],label=r'$Pr[x|s_0]$',alpha=0.3,color='tab:orange')\n",
    "    plt.plot(x_range,rdist['x_1'],label=r'$Pr[x|s_1]$',alpha=0.3,color='#9f86c0')\n",
    "    plt.ylabel('PMF',fontsize=14)\n",
    "    plt.xlabel(r'$supp(X)=supp(\\tilde{X})$',fontsize=20)\n",
    "    plt.legend()\n",
    "    return plt\n",
    "\n",
    "# def DisparateImpact(X_test,y_test,y_pred):\n",
    "#     df_test=pd.DataFrame(np.concatenate((X_test,y_test.reshape(-1,1),y_pred.reshape(-1,1)), axis=1),columns=['X','S','W','Y','f'])\n",
    "#     numerator=sum(df_test[(df_test['S']==0)&(df_test['f']==1)&(df_test['Y']==1)]['W'])/sum(df_test[(df_test['S']==0)&(df_test['Y']==1)]['W'])\n",
    "#     denominator=sum(df_test[(df_test['S']==1)&(df_test['f']==1)&(df_test['Y']==1)]['W'])/sum(df_test[(df_test['S']==1)&(df_test['Y']==1)]['W'])\n",
    "#     return numerator/denominator\n",
    "\n",
    "# def DisparateImpact(X_test,y_pred):\n",
    "#     df_test=pd.DataFrame(np.concatenate((X_test,y_pred.reshape(-1,1)), axis=1),columns=['X','S','W','f'])\n",
    "#     numerator=sum(df_test[(df_test['S']==0)&(df_test['f']==1)]['W'])/sum(df_test[df_test['S']==0]['W'])\n",
    "#     denominator=sum(df_test[(df_test['S']==1)&(df_test['f']==1)]['W'])/sum(df_test[df_test['S']==1]['W'])\n",
    "#     if numerator==denominator:\n",
    "#         return 1\n",
    "#     return numerator/denominator\n",
    "\n",
    "def DisparateImpact_postprocess(df_test,y_pred_tmp):\n",
    "    df_test_tmp=df_test[:]\n",
    "    df_test_tmp.insert(loc=0, column='f', value=y_pred_tmp)\n",
    "    numerator=sum(df_test_tmp[(df_test_tmp['S']==0)&(df_test_tmp['f']==1)]['W'])/sum(df_test_tmp[df_test_tmp['S']==0]['W'])\n",
    "    denominator=sum(df_test_tmp[(df_test_tmp['S']==1)&(df_test_tmp['f']==1)]['W'])/sum(df_test_tmp[df_test_tmp['S']==1]['W'])\n",
    "    # if numerator==denominator:\n",
    "    #     return 1\n",
    "    return numerator/denominator\n",
    "\n",
    "def rdata_analysis(rdata,x_range,x_name):\n",
    "    rdist=dict()\n",
    "    pivot=pd.pivot_table(rdata,index=x_name,values=['W'],aggfunc=[np.sum])[('sum','W')]\n",
    "    rdist['x']= np.array([pivot[i] for i in x_range])/sum([pivot[i] for i in x_range]) #empirical_distribution(rdata,x_range)\n",
    "    if rdata[rdata['S']==0].shape[0]>0:\n",
    "        pivot0=pd.pivot_table(rdata[rdata['S']==0],index=x_name,values=['W'],aggfunc=[np.sum])[('sum','W')]\n",
    "        rdist['x_0']=np.array([pivot0[i] if i in list(pivot0.index) else 0 for i in x_range])/sum([pivot0[i] if i in list(pivot0.index) else 0 for i in x_range]) #empirical_distribution(rdata[rdata['S']==0],x_range)\n",
    "    if rdata[rdata['S']==1].shape[0]>0:\n",
    "        pivot1=pd.pivot_table(rdata[rdata['S']==1],index=x_name,values=['W'],aggfunc=[np.sum])[('sum','W')]\n",
    "        rdist['x_1']=np.array([pivot1[i] if i in list(pivot1.index) else 0 for i in x_range])/sum([pivot1[i] if i in list(pivot1.index) else 0 for i in x_range]) #empirical_distribution(rdata[rdata['S']==1],x_range)\n",
    "    return rdist\n",
    "\n",
    "def c_generate_higher(x_range,weight):\n",
    "    bin=len(x_range)\n",
    "    dim=len(x_range[0])\n",
    "    C=np.random.random((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            C[i,j]=sum(weight[d]*abs(x_range[i][d]-x_range[j][d]) for d in range(dim))\n",
    "    return C\n",
    "\n",
    "def c_generate(x_range):\n",
    "    bin=len(x_range)\n",
    "    C=np.random.random((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            C[i,j]=abs(x_range[i]-x_range[j])\n",
    "    return C\n",
    "\n",
    "def projection_higher(df,coupling_matrix,x_range,x_list,var_list):\n",
    "    df=df.drop(columns=x_list)\n",
    "    dim=len(x_list)\n",
    "    bin=len(x_range)\n",
    "    arg_list=[elem for elem in var_list if elem not in x_list]\n",
    "    df=df[arg_list+['X','S','W','Y']]\n",
    "    coupling=coupling_matrix.A1.reshape((bin,bin))\n",
    "    df_t=pd.DataFrame(columns=arg_list+['X','S','W','Y'])\n",
    "    for i in range(df.shape[0]):\n",
    "        orig=df.iloc[i]\n",
    "        loc=np.where([x_range[b]==orig['X'] for b in range(bin)])[0][0]\n",
    "        #rows=np.nonzero(coupling[loc,:])[0]\n",
    "        sub_dict={'X':x_range,'W':list(coupling[loc,:]/(sum(coupling[loc,:]))*orig['W'])}\n",
    "        sub_dict.update({var:[orig[var]]*bin for var in arg_list+['S','Y']})\n",
    "        sub=pd.DataFrame(data=sub_dict, index=[*range(bin)])\n",
    "        df_t=pd.concat([df_t,sub],ignore_index=True)#pd.concat([df_t,samples_groupby(sub,x_list)], ignore_index=True)\n",
    "    return df_t\n",
    "    df_t=df_t.groupby(by=arg_list+['X','S','Y'],as_index=False).sum()\n",
    "    for d in range(dim):\n",
    "        df_t[x_list[d]]=[df_t['X'][r][d] for r in range(df_t.shape[0])]\n",
    "    return df_t[var_list+['S','W','Y']]\n",
    "\n",
    "# def postprocess(df_test,coupling_matrix,x_range,x_range_pred):\n",
    "#     bin=len(x_range)\n",
    "#     coupling=coupling_matrix.A1.reshape((bin,bin))\n",
    "#     x_pred_repaired=[]\n",
    "#     for loc in range(bin):\n",
    "#         ##loc=np.where([x_range[i]==orig['X'] for i in range(bin)])[0][0]\n",
    "#         rows=np.nonzero(coupling[loc,:])[0]\n",
    "#         p0=sum(x_range_pred[:,0]*coupling[loc,rows]/(sum(coupling[loc,rows])))\n",
    "#         p1=sum(x_range_pred[:,1]*coupling[loc,rows]/(sum(coupling[loc,rows])))\n",
    "#         if p0>p1:\n",
    "#             x_pred_repaired+=[0]\n",
    "#         else:\n",
    "#             x_pred_repaired+=[1]\n",
    "#     pred_dict=dict(zip(x_range,x_pred_repaired))\n",
    "#     f_repaired=[pred_dict[df_test['X'][i]] for i in range(df_test.shape[0])]\n",
    "#     return np.array(f_repaired)\n",
    "\n",
    "def postprocess(df,coupling_matrix,x_list,x_range,var_list,var_range,clf):\n",
    "    dim=len(x_list)\n",
    "    var_dim=len(var_list)\n",
    "    bin=len(x_range)\n",
    "    x_loc_dict=dict(zip(x_range,[*range(bin)]))\n",
    "    arg_list=[elem for elem in var_list if elem not in x_list]\n",
    "    coupling=coupling_matrix.A1.reshape((bin,bin))\n",
    "    pred_repaired=dict()\n",
    "    for i in range(len(var_range)):\n",
    "        var_tmp=pd.Series({var_list[d]:var_range[i][d] for d in range(var_dim)})\n",
    "        if dim>1:\n",
    "            loc=x_loc_dict[tuple(var_tmp[x_list])]\n",
    "        else: \n",
    "            loc=x_loc_dict[var_tmp[x_list[0]]]\n",
    "        sub=pd.DataFrame(x_range,columns=x_list)\n",
    "        for arg in arg_list:\n",
    "            sub[arg]=var_tmp[arg] \n",
    "        sub=sub[var_list]\n",
    "        prob=np.array([clf.predict_proba(np.array(sub.loc[r]).reshape(-1,var_dim))[0] for r in range(bin)])\n",
    "        totalweight=sum(coupling[loc,:])\n",
    "        prob0=sum(prob[:,0]*coupling[loc,:]/totalweight)\n",
    "        prob1=sum(prob[:,1]*coupling[loc,:]/totalweight)\n",
    "        pred_repaired.update({var_range[i]:int(prob0<prob1)})\n",
    "    return np.array([pred_repaired[tuple(df[var_list].iloc[i])] for i in range(df.shape[0])])\n",
    "\n",
    "def postprocess_bary(df,coupling_bary_matrix,x_list,x_range,var_list,var_range,clf):\n",
    "    bin=len(x_range)\n",
    "    coupling_bary=coupling_bary_matrix.A1.reshape((bin,bin))\n",
    "    s0=df[df['S']==0]\n",
    "    s1=df[df['S']==1]\n",
    "    pi0=s0.shape[0]/df.shape[0]\n",
    "    pi1=s1.shape[0]/df.shape[0]\n",
    "    coupling0=np.zeros((bin,bin))\n",
    "    coupling1=np.zeros((bin,bin))\n",
    "    for i in range(bin):\n",
    "        for j in range(bin):\n",
    "            # assume the distance between every two adjacent x indices is the same\n",
    "            ind0=int(pi0*i+pi1*j)\n",
    "            ind1=int(pi0*j+pi1*i)\n",
    "            coupling0[i,ind0]+=coupling_bary[i,j]\n",
    "            coupling1[i,ind1]+=coupling_bary[j,i]\n",
    "\n",
    "    # assess if dist['td{x}_0']==dist['td{x}_1']\n",
    "    projectedDist_s0=rdata_analysis(projection_higher(s0,np.matrix(coupling0),x_range,x_list,var_list),x_range,'X')['x_0']\n",
    "    projectedDist_s1=rdata_analysis(projection_higher(s1,np.matrix(coupling1),x_range,x_list,var_list),x_range,'X')['x_1']\n",
    "    print('tv distance between projected S-wise distributions',sum(abs(projectedDist_s0-projectedDist_s1))/2)\n",
    "\n",
    "    s0.insert(loc=0, column='f', value=postprocess(s0,np.matrix(coupling0),x_list,x_range,var_list,var_range,clf))\n",
    "    s1.insert(loc=0, column='f', value=postprocess(s1,np.matrix(coupling1),x_list,x_range,var_list,var_range,clf))\n",
    "    s_concate=pd.concat([s0,s1], ignore_index=False)\n",
    "    s_concate.sort_index()\n",
    "    return np.array(s_concate['f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender wise\n",
    "print(pa)\n",
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "    tv_dist[x_name]=round(sum(abs(dist['x_0']-dist['x_1']))/2,4)\n",
    "tv_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#racewise\n",
    "print(pa)\n",
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "    tv_dist[x_name]=round(sum(abs(dist['x_0']-dist['x_1']))/2,4)\n",
    "tv_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hoursperweek', 'age']\n"
     ]
    }
   ],
   "source": [
    "K=200\n",
    "e=0.01\n",
    "\n",
    "var_list=['hoursperweek','age','capitalgain','capitalloss' ,'education-num'] #class must be in the end ,,'capitalloss' ,'education-num'\n",
    "var_dim=len(var_list)\n",
    "pa='sex'\n",
    "pa_dict={'Male':1,'Female':0,'White':1,'Black':0}\n",
    "\n",
    "messydata=pd.read_csv('C:/Users/zhouq/Documents/optimal_transport/adult_csv.csv',usecols=var_list+[pa,'class'])\n",
    "messydata=messydata.rename(columns={pa:'S','class':'Y'})\n",
    "messydata['S']=messydata['S'].replace(pa_dict)\n",
    "messydata['Y']=messydata['Y'].replace({'>50K':1,'<=50K':0})\n",
    "messydata=messydata[(messydata['S']==1)|(messydata['S']==0)]\n",
    "for col in var_list+['S','Y']:\n",
    "    messydata[col]=messydata[col].astype('category')\n",
    "messydata['W']=1\n",
    "X=messydata[var_list+['S','W']].to_numpy() # [X,S,W]\n",
    "y=messydata['Y'].to_numpy() #[Y]\n",
    "\n",
    "tv_dist=dict()\n",
    "for x_name in var_list:\n",
    "    x_range_single=list(pd.pivot_table(messydata,index=x_name,values=['W'])[('W')].index) \n",
    "    dist=rdata_analysis(messydata,x_range_single,x_name)\n",
    "    tv_dist[x_name]=sum(abs(dist['x_0']-dist['x_1']))/2\n",
    "x_list=[]\n",
    "for key,val in tv_dist.items():\n",
    "    if val>0.1:\n",
    "        x_list+=[key]\n",
    "print(x_list)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "clf=RandomForestClassifier(max_depth=5, random_state=0).fit(X_train[:,0:var_dim],y_train)\n",
    "df_test=pd.DataFrame(np.concatenate((X_test,y_test.reshape(-1,1)), axis=1),columns=var_list+['S','W','Y'])\n",
    "df_test=df_test.groupby(by=var_list+['S','Y'],as_index=False).sum()\n",
    "if len(x_list)>1:\n",
    "    df_test['X']=[tuple(df_test[x_list].values[r]) for r in range(df_test.shape[0])]\n",
    "    x_range=list(set(df_test['X']))\n",
    "    weight=list(1/(df_test[x_list].max()-df_test[x_list].min())) # because 'education-num' range from 1 to 16 while others 1 to 4\n",
    "    C=c_generate_higher(x_range,weight)\n",
    "else:\n",
    "    df_test['X']=df_test[x_list[0]]\n",
    "    x_range=list(set(df_test['X']))\n",
    "    C=c_generate(x_range)\n",
    "\n",
    "bin=len(x_range)\n",
    "var_range=list(pd.pivot_table(df_test,index=var_list,values=['S','W','Y']).index)\n",
    "dist=rdata_analysis(df_test,x_range,'X')\n",
    "dist['t_x']=dist['x'] # #dist['x'] #dist['x_0']*0.5+dist['x_1']*0.5 \n",
    "dist['v']=[(dist['x_0'][i]-dist['x_1'][i])/dist['x'][i] for i in range(bin)]\n",
    "\n",
    "px=np.matrix(dist['x']).T\n",
    "ptx=np.matrix(dist['t_x']).T\n",
    "if np.any(dist['x_0']==0):\n",
    "    p0=np.matrix((dist['x_0']+10e-9)/sum(dist['x_0']+10e-9)).T\n",
    "else:\n",
    "    p0=np.matrix(dist['x_0']).T \n",
    "if np.any(dist['x_1']==0):\n",
    "    p1=np.matrix((dist['x_1']+10e-9)/sum(dist['x_1']+10e-9)).T\n",
    "else:\n",
    "    p1=np.matrix(dist['x_1']).T \n",
    "V=np.matrix(dist['v']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of violation of f: [[0.00335028]]\n",
      "sum of violation of g: [[1.86049093e-16]]\n",
      "total cost: 0.20207238097846492\n",
      "entropy: 3.336702541170853\n",
      "tr violation: [[0.22035738]]\n",
      "============================================\n",
      "tv distance between projected S-wise distributions 0.0016676258738170914\n"
     ]
    }
   ],
   "source": [
    "y_pred_bary=postprocess_bary(df_test,baseline(C,e,p0,p1,V,K)[K*2],x_list,x_range,var_list,var_range,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of violation of f: [[2.04708941e-08]]\n",
      "sum of violation of g: [[1.49186219e-16]]\n",
      "total cost: 1.8177171206188376e-08\n",
      "entropy: 2.7163083347438826\n",
      "tr violation: [[0.41077795]]\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "y_pred_base=postprocess(df_test,baseline(C,e,px,ptx,V,K)[K*2],x_list,x_range,var_list,var_range,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of violation of f: [[0.00634802]]\n",
      "sum of violation of g: [[0.00022632]]\n",
      "total cost: 0.24722451853548127\n",
      "entropy: 3.5149255350857382\n",
      "tr violation: [[0.02498012]]\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "y_pred_part2=postprocess(df_test,partial_repair(C,e,px,ptx,V,1.0e-3,K)[K*3],x_list,x_range,var_list,var_range,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(np.array(df_test[var_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred),\n",
    "                    'f1 macro':f1_score(df_test['Y'], y_pred, average='macro',sample_weight=df_test['W']),\n",
    "                    'f1 micro':f1_score(df_test['Y'], y_pred, average='micro',sample_weight=df_test['W']),\n",
    "                    'f1 weighted':f1_score(df_test['Y'], y_pred, average='weighted',sample_weight=df_test['W']),\n",
    "                    'method':'origin'})\n",
    "new_row_base=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_base),\n",
    "                    'f1 macro':f1_score(df_test['Y'], y_pred_base, average='macro',sample_weight=df_test['W']),\n",
    "                    'f1 micro':f1_score(df_test['Y'], y_pred_base, average='micro',sample_weight=df_test['W']),\n",
    "                    'f1 weighted':f1_score(df_test['Y'], y_pred_base, average='weighted',sample_weight=df_test['W']),\n",
    "                    'method':'baseline'})\n",
    "new_row_bary=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_bary),\n",
    "                    'f1 macro':f1_score(df_test['Y'], y_pred_bary, average='macro',sample_weight=df_test['W']),\n",
    "                    'f1 micro':f1_score(df_test['Y'], y_pred_bary, average='micro',sample_weight=df_test['W']),\n",
    "                    'f1 weighted':f1_score(df_test['Y'], y_pred_bary, average='weighted',sample_weight=df_test['W']),\n",
    "                    'method':'barycentre'})\n",
    "new_row_part2=pd.Series({'DI':DisparateImpact_postprocess(df_test,y_pred_part2),\n",
    "                    'f1 macro':f1_score(df_test['Y'], y_pred_part2, average='macro',sample_weight=df_test['W']),\n",
    "                    'f1 micro':f1_score(df_test['Y'], y_pred_part2, average='micro',sample_weight=df_test['W']),\n",
    "                    'f1 weighted':f1_score(df_test['Y'], y_pred_part2, average='weighted',sample_weight=df_test['W']),\n",
    "                    'method':'partial repair2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DI             0.412847\n",
      "f1 macro       0.684252\n",
      "f1 micro       0.817307\n",
      "f1 weighted    0.789792\n",
      "method           origin\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DI             0.412847\n",
      "f1 macro       0.684252\n",
      "f1 micro       0.817307\n",
      "f1 weighted    0.789792\n",
      "method         baseline\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_row_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DI                0.87882\n",
      "f1 macro         0.522604\n",
      "f1 micro          0.67563\n",
      "f1 weighted      0.661777\n",
      "method         barycentre\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_row_bary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DI                    0.481826\n",
      "f1 macro              0.629819\n",
      "f1 micro              0.806251\n",
      "f1 weighted           0.761411\n",
      "method         partial repair2\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(new_row_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report=pd.read_csv('C:/Users/zhouq/Documents/optimal_transport/report_postprocess_sex.csv')\n",
    "performance=list(report.columns)[:-1]\n",
    "methods=['origin','baseline','partial_repair2','partial_repair3'] #,'partial_repair4'\n",
    "#list(set(report['method']))\n",
    "colors=['#5f0f40','#9a031e','#FF8811','#F4D06F','#9DD9D2']\n",
    "pivot=pd.pivot_table(report,index=['method'],values=performance,aggfunc=[np.mean,np.std])\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=np.arange(len(performance))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "i=0\n",
    "for m in methods:\n",
    "    sub=[pivot[('mean',p)][m] for p in performance]\n",
    "    err=[pivot[('std',p)][m] for p in performance]\n",
    "    ax1.bar(ind+width*i,sub,width,yerr=err,color=colors[i],label=m)\n",
    "    i+=1\n",
    "    \n",
    "ax1.legend(['Origin','Baseline',r'$10^{-2}$-repair',r'$10^{-3}$-repair',r'$5\\times 10^{-4}$-repair'],fontsize=14,\n",
    "            framealpha=0.2,bbox_to_anchor=(1.03,1.1),ncol=5,frameon=False)\n",
    "ax1.set_xlabel('Indices', fontsize=20)\n",
    "ax1.set_ylabel('Values', fontsize=20)\n",
    "\n",
    "ax1.set_xticks(ind+width*2)\n",
    "ax1.set_xticklabels(['DisparateImpact']+[i for i in performance[1:]], fontsize=16)\n",
    "ax1.tick_params(axis='y', which='major', labelsize=14)\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "#plt.savefig(\"C:/Users/zhouq/Documents/optimal_transport/adult_higher.pdf\",bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
